---
title: "Concurrent Food vs. Social Choice"
author: "Greg Jensen"
date: "6/15/2020"
output: html_notebook
---

*Stan Analysis of Food/Social Choice*

```{r set up, include=FALSE}
library(rethinking)
library(readr)
library(ggplot2)
library(rstan)
library(dplyr)
library(tidyr)
setwd("~/Documents/Google Drive/Reed/Learning & Adaptive Behavior Laboratory/Sharing Data - Matt")

Behav <- read_csv("Raw_data.csv")
Behav$Subject <- coerce_index(Behav$Rat)

rstan_options(auto_write = TRUE);
options(mc.cores = parallel::detectCores());
```

This block of code compares the binary feed/social choices using a binomial likelihood, which is effectively a logistic regression. This model is fit for each subject independently, and then the mean parameters are evaluated after the fact as the mean of the subject-level parameters. Importantly, this is NOT a multi-level model. However, the models involved in this approach are easier to understand, so it's probably an easier starting approach.

First, we need to compile our Stan model for pairwise data.

```{r}
model1 <- stan_model(file="pair_model.stan")
```

Since we are fitting one model for each subject, we need to extract a subset of data for each of the subjects, fit that subject's model, and save the result. to this end, we'll create a list called choice_output that stores the full Stan model for each subject and another list called cq that stores the extracted parameter samples generated by those models.

```{r}
choice_output <- list()
sample_parameter_1 <- list()
```

cyc governs how many samples the sampler will generate for each of its chains. With 4 chains, we'll get a total of $4 /times 2000=8000$ post-warmup samples for each parameter, which should give us a good estimate of not only its mean but also its dispersion. This is *probably* overkill, but better to have too many samples than too few.

```{r}
nsample <- 2000
for (s in unique(Behav$Subject)) {
  mdata_1 <- (Behav$Condition != 5) & (Behav$Subject == s)
  mdata_2 <- list(S = sum(mdata_1), C = length(unique(Behav$Condition[mdata_1])), N = Behav$Social[mdata_1] + Behav$Food[mdata_1], condID = Behav$Condition[mdata_1], F = Behav$Food[mdata_1])
  mdata_2$condID[mdata_2$condID > 5] <- mdata_2$condID[mdata_2$condID > 5] - 1
  choice_output[s] <- list(sampling(model1, data = mdata_2, iter = nsample*2, warmup = nsample, chains = 4, cores = 1, control=list(adapt_delta = 0.99, max_treedepth = 15)))
  sample_parameter_1[s] <- list(extract.samples(choice_output[[s]]))
  sample_parameter_1[[s]]$prob <- inv_logit(sample_parameter_1[[s]]$intercept)
}
```

Within the loop command, the first line omits condition 5, since the Sharing choice wasn't available to subjects in that case. 

"dat" in the second line of command is the data subset that will be fed to the sampler. It's absolutely crucial that each of the variables in dat has the same name and data properties as those specified in the data block of the Stan model; otherwise, the sampler will throw an error and refuse to fit the model. 

For the third line of code, since we're skipping over condition 5, we're going to omit it from our indexing, resulting in 6 conditions in total. 

The fourth line of code runs the sampler and pushed the result into the appropriate slot in the list of outputs. Here, I am running the model across multiple cores because it's good practice to work with multiple chains, each of which has a different starting position, and if you're going to run the model multiple times, you may as well hand each task to a different processor and let them work in parallel. The "control" list is included to force the model to run each sampler to a high level of precision. This appreciably slows the sampler, but also helps to mitigate the risk of divergent samples. Getting a handful of divergent samples is no bit deal, but if you're getting dozens or hundreds of divergent samples, it's a sign that something is probably wrong with your model, such as redundant parameters that result in an overspecified model.

The full Stan output is a big bulky object that is difficult to work with. In practice, we mainly care about the posterior samples, so each bundle of samples is passed to its own container here.

We'll go ahead and generate the probabilities that are implied by each intercept distribution here while we're at it, reversing the implicit logit transform that the model employs to help things run smoothly.


```{r}
sample_parameter_1[s+1] <- list(extract.samples(choice_output[[s]]))
sample_parameter_1[[s+1]]$intercept <- matrix(0,nrow(sample_parameter_1[[1]]$intercept),ncol(sample_parameter_1[[1]]$intercept))
for (k in 1:s) {
  sample_parameter_1[[s+1]]$intercept <- sample_parameter_1[[s+1]]$intercept + sample_parameter_1[[k]]$intercept/s
}
```

This bit probably seems counterintuitive, but what it's doing is that it's making a copy of cq[[s]] in the final position of cq, then replacing all the parameters with the mean values of those parameters across all subjects. This is the maneuver that will give you your means for each condition while retaining the same data structure as the subject-level estimates.

```{r}
sample_parameter_1[[s+1]]$prob <- inv_logit(sample_parameter_1[[s+1]]$intercept)
```

Here, the probabilities are derived from the mean intercepts. This is almost always better than looking at the mean of the probabilities! inv_logit(mean(intercept)) is centered with respect to a symmetrical distribution, whereas mean(inv_logit(intercept)) is going to skew toward chance because probability is bounded.

```{r}
plotdata <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  intercept = c( sample_parameter_1[[s+1]]$intercept[,1], sample_parameter_1[[s+1]]$intercept[,2], sample_parameter_1[[s+1]]$intercept[,3], sample_parameter_1[[s+1]]$intercept[,4], sample_parameter_1[[s+1]]$intercept[,5], sample_parameter_1[[s+1]]$intercept[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=intercept, fill=name)) +
  geom_violin()
print(p)

plotdata <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  proportion = c( sample_parameter_1[[s+1]]$prob[,1], sample_parameter_1[[s+1]]$prob[,2], sample_parameter_1[[s+1]]$prob[,3], sample_parameter_1[[s+1]]$prob[,4], sample_parameter_1[[s+1]]$prob[,5], sample_parameter_1[[s+1]]$prob[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=proportion, fill=name)) +
  geom_violin(scale="width") +
  ylim(0.35, 1)
print(p)
```


With that, the process of performaning the statistical inference is technically finished. The descriptive statistics of the posterior samples are analogous to statements about that distribution, so plotting those samples, or a boxplot described from their values, does the job of reporting the results to readers. 



**Rat 4**

```{r}
plotdata_4 <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  intercept = c( sample_parameter[[1]]$intercept[,1], sample_parameter[[1]]$intercept[,2], sample_parameter[[1]]$intercept[,3], sample_parameter[[1]]$intercept[,4], sample_parameter[[1]]$intercept[,5], sample_parameter[[1]]$intercept[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=intercept, fill=name)) +
  geom_violin()
print(p)

plotdata_4 <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  proportion = c( sample_parameter[[1]]$prob[,1], sample_parameter[[1]]$prob[,2], sample_parameter[[1]]$prob[,3], sample_parameter[[1]]$prob[,4], sample_parameter[[1]]$prob[,5], sample_parameter[[1]]$prob[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=proportion, fill=name)) +
  geom_violin(scale="width") +
  ylim(0.35, 1)
print(p)
```


**Rat 6**

```{r}
plotdata_6 <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  intercept = c( sample_parameter[[2]]$intercept[,1], sample_parameter[[2]]$intercept[,2], sample_parameter[[2]]$intercept[,3], sample_parameter[[2]]$intercept[,4], sample_parameter[[2]]$intercept[,5], sample_parameter[[2]]$intercept[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=intercept, fill=name)) +
  geom_violin()
print(p)

plotdata_6 <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  proportion = c( sample_parameter[[2]]$prob[,1], sample_parameter[[2]]$prob[,2], sample_parameter[[2]]$prob[,3], sample_parameter[[2]]$prob[,4], sample_parameter[[2]]$prob[,5], sample_parameter[[2]]$prob[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=proportion, fill=name)) +
  geom_violin(scale="width") +
  ylim(0.3, 1)
print(p)
```

**Rat 8**

```{r}
plotdata_8 <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  intercept = c( sample_parameter[[3]]$intercept[,1], sample_parameter[[3]]$intercept[,2], sample_parameter[[3]]$intercept[,3], sample_parameter[[3]]$intercept[,4], sample_parameter[[3]]$intercept[,5], sample_parameter[[3]]$intercept[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=intercept, fill=name)) +
  geom_violin()
print(p)

plotdata_8 <- data.frame(
  name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
  proportion = c( sample_parameter[[3]]$prob[,1], sample_parameter[[3]]$prob[,2], sample_parameter[[3]]$prob[,3], sample_parameter[[3]]$prob[,4], sample_parameter[[3]]$prob[,5], sample_parameter[[3]]$prob[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=proportion, fill=name)) +
  geom_violin(scale="width") +
  ylim(0.38, 1)
print(p)
```




**Stan Analysis of Food Intake By Type**


For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.

```{r}

model2 <- stan_model(file="intake_model.stan")

intake_output <- list()
sample_parameter_2 <- list()
nsample <- 2000
for (s in unique(Behav$Subject)) {
  mdata_1 <- (Behav$Subject == s)
  mdata_2 <- list(S = sum(mdata_1), C = length(unique(Behav$Condition[mdata_1])), condID = Behav$Condition[mdata_1], P = Behav$Food[mdata_1] * Behav$FoodAmmt[mdata_1], Shr=Behav$Sharing[mdata_1], Lft = Behav$PelletsLeft[mdata_1])
  mdata_2$Shr[mdata_2$Shr<0] <- 0
  mdata_2$Lft[mdata_2$Lft<0] <- 0
  mdata_2$NP <- mdata_2$Shr + mdata_2$Lft
  intake_output[s] <- list(sampling(model2, data = mdata_2, iter = nsample * 2, warmup = nsample, chains = 4, cores = 1, control = list(adapt_delta = 0.99, max_treedepth = 15)))
  sample_parameter_2[s] <- list(extract.samples(intake_output[[s]]))
  sample_parameter_2[[s]]$sum <- sample_parameter_2[[s]]$intercept + sample_parameter_2[[s]]$self
  sample_parameter_2[[s]]$pellets_left <- exp(sample_parameter_2[[s]]$intercept)
  sample_parameter_2[[s]]$pellets_produce <- exp(sample_parameter_2[[s]]$sum)
}

```

As noted in the model, NP is the sum of shared and 'left behind' pellets. Since these two categories of event are non-overlapping across conditions, we don't run any risk in pooling them as a global event and then teasing the implications apart when we label the results.

Since intercept[i] and self[i] covary, we should NOT interpret each separately. A much safer approach is to create a derived value here called "is" that assembles the parameters in the way they are assembled by the model. This then lets us obtain "pellets_left" and "pellets_taken" as quantities that correct for the implicit transformation of the parameters in the model.

As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so long as subjects were independent samples. It's also much easier to understand the model as it applies to single subjects.

```{r}
sample_parameter_2[s+1] <- list(extract.samples(intake_output[[s]]))
sample_parameter_2[[s+1]]$intercept <- matrix(0, nrow(sample_parameter_2[[1]]$intercept), ncol(sample_parameter_2[[1]]$intercept))
sample_parameter_2[[s+1]]$self <- matrix(0, nrow(sample_parameter_2[[1]]$self), ncol(sample_parameter_2[[1]]$self))
sample_parameter_2[[s+1]]$sum <- matrix(0, nrow(sample_parameter_2[[1]]$sum), ncol(sample_parameter_2[[1]]$sum))

for (k in 1:s) {
  sample_parameter_2[[s+1]]$intercept <- sample_parameter_2[[s+1]]$intercept + sample_parameter_2[[k]]$intercept/s
  sample_parameter_2[[s+1]]$self <- sample_parameter_2[[s+1]]$self + sample_parameter_2[[k]]$self/s
  sample_parameter_2[[s+1]]$sum <- sample_parameter_2[[s+1]]$sum + sample_parameter_2[[k]]$sum/s
}
sample_parameter_2[[s+1]]$pellets_left <- exp(sample_parameter_2[[s+1]]$intercept)
sample_parameter_2[[s+1]]$pellets_produce <- exp(sample_parameter_2[[s+1]]$sum)

plotdata <- data.frame(
  name = c( rep("Cond 1 NP",8000), rep("Cond 1 P",8000), rep("Cond 2 NP",8000), rep("Cond 2 P",8000), rep("Cond 3 NP",8000), rep("Cond 3 P",8000), rep("Cond 4 NP",8000), rep("Cond 4 P",8000), rep("Cond 5 NP",8000), rep("Cond 5 P",8000), rep("Cond 6 NP",8000), rep("Cond 6 P",8000), rep("Cond 7 NP",8000), rep("Cond 7 P",8000) ),
  intercept = c( sample_parameter_2[[s+1]]$intercept[,1], sample_parameter_2[[s+1]]$sum[,1], sample_parameter_2[[s+1]]$intercept[,2], sample_parameter_2[[s+1]]$sum[,2], sample_parameter_2[[s+1]]$intercept[,3], sample_parameter_2[[s+1]]$sum[,3], sample_parameter_2[[s+1]]$intercept[,4], sample_parameter_2[[s+1]]$sum[,4], sample_parameter_2[[s+1]]$intercept[,5], sample_parameter_2[[s+1]]$sum[,5], sample_parameter_2[[s+1]]$intercept[,6], sample_parameter_2[[s+1]]$sum[,6], sample_parameter_2[[s+1]]$intercept[,7], sample_parameter_2[[s+1]]$sum[,7] ),
  condition = c( rep("Cond 1",16000), rep("Cond 2",16000), rep("Cond 3",16000), rep("Cond 4",16000), rep("Cond 5",16000), rep("Cond 6",16000), rep("Cond 7",16000) )
)
p <- ggplot(plotdata, aes(x=name, y=intercept, fill=condition)) +
  geom_violin(scale="width")
print(p)

plotdata <- data.frame(
  name = c( rep("Cond 1 NP",8000), rep("Cond 1 P",8000), rep("Cond 2 NP",8000), rep("Cond 2 P",8000), rep("Cond 3 NP",8000), rep("Cond 3 P",8000), rep("Cond 4 NP",8000), rep("Cond 4 P",8000), rep("Cond 5 NP",8000), rep("Cond 5 P",8000), rep("Cond 6 NP",8000), rep("Cond 6 P",8000), rep("Cond 7 NP",8000), rep("Cond 7 P",8000) ),
  pellets = c( sample_parameter_2[[s+1]]$pellets_left[,1], sample_parameter_2[[s+1]]$pellets_taken[,1], sample_parameter_2[[s+1]]$pellets_left[,2], sample_parameter_2[[s+1]]$pellets_taken[,2], sample_parameter_2[[s+1]]$pellets_left[,3], sample_parameter_2[[s+1]]$pellets_taken[,3], sample_parameter_2[[s+1]]$pellets_left[,4], sample_parameter_2[[s+1]]$pellets_taken[,4], sample_parameter_2[[s+1]]$pellets_left[,5], sample_parameter_2[[s+1]]$pellets_taken[,5], sample_parameter_2[[s+1]]$pellets_left[,6], sample_parameter_2[[s+1]]$pellets_taken[,6], sample_parameter_2[[s+1]]$pellets_left[,7], sample_parameter_2[[s+1]]$pellets_taken[,7] ),
  condition = c( rep("Cond 1",16000), rep("Cond 2",16000), rep("Cond 3",16000), rep("Cond 4",16000), rep("Cond 5",16000), rep("Cond 6",16000), rep("Cond 7",16000) )
)
p <- ggplot(plotdata, aes(x=name, y=pellets, fill=condition)) +
  geom_violin(scale="width")
print(p)
```









Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

