geom_violin()
print(p)
plotdata <- data.frame(
name = c( rep("Cond 1",8000), rep("Cond 2",8000), rep("Cond 3",8000), rep("Cond 4",8000), rep("Cond 6",8000), rep("Cond 7",8000) ),
proportion = c( cq[[s+1]]$prob[,1],cq[[s+1]]$prob[,2],cq[[s+1]]$prob[,3],cq[[s+1]]$prob[,4],cq[[s+1]]$prob[,5],cq[[s+1]]$prob[,6] )
)
p <- ggplot(plotdata, aes(x=name, y=proportion, fill=name)) +
geom_violin(scale="width")
print(p)
intake_machine_p <- stan_model(file="intake_model.stan")
intake_output <- list()
iq <- list()
cyc <- 2000
for (s in unique(Behav$Subject)) {
dex <- (Behav$Subject==s)
dat <- list(S=sum(dex),C=length(unique(Behav$Condition[dex])),condID=Behav$Condition[dex],P=Behav$Food[dex]*Behav$FoodAmmt[dex],Shr=Behav$Sharing[dex],Lft=Behav$PelletsLeft[dex])
dat$Shr[dat$Shr<0] <- 0
dat$Lft[dat$Lft<0] <- 0
dat$NP <- dat$Shr + dat$Lft
intake_output[s] <- list(sampling(intake_machine_p, data=dat, iter=cyc*2, warmup=cyc, chains=4, cores=4, control=list(adapt_delta=0.99,max_treedepth=15)))
iq[s] <- list(extract.samples(intake_output[[s]]))
iq[[s]]$is <- iq[[s]]$intercept + iq[[s]]$self
iq[[s]]$pellets_left <- exp(iq[[s]]$intercept)
iq[[s]]$pellets_taken <- exp(iq[[s]]$is)
}
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
coerce_index(Behav$Rat)
install.packages('rethinking',type='source'))install.packages(c('devtools','coda','mvtnorm')) library(devtools) install_github("rmcelreath/rethinking")
install.packages('rethinking',type='source')
install.packages("rethinking", type = "source")
library(rethinking)
install.packages(c("ggplot2", "readr"))
library(rethinking)
# Run all of the following code, either by selecting all the code and
# pressing the "Run" button (if you are using RStudio) or by copying
# the code and pasting it into the R console and pressing return.
# This bit of code installs the "rstan" package, which is essential to
# the Stan programming language that will do the heavy lifting in the
# background later in the course. Without running this installer first,
# the "rethinking" package won't work properly.
remove.packages("rstan")
install.packages("StanHeaders")
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
# Once "rstan" is installed, we can install the "rethinking" package
# proper, drawn directly from the author's website.
install.packages(c("mvtnorm","loo","coda"), repos="https://cloud.r-project.org/",dependencies=TRUE)
install.packages('rethinking',type='source')
library("rstan")
library("rethinking")
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
# If everything has gone according to plan, you should now be able
# to to use the full scope of the tools in the rethinking package.
# If you run into difficulty, let me know ASAP so we can
# troubleshoot the problem.
library(rethinking)
install.packages('rethinking',type='source'))install.packages(c('devtools','coda','mvtnorm')) library(devtools) install_github("rmcelreath/rethinking"))
install.packages('rethinking',type='source'))
install.packages('rethinking',type='source')
install.packages(c('devtools','coda','mvtnorm'))
install.packages(c('coda','mvtnorm')) options(repos=c(getOption('repos'),rethinking='http://xcelab.net/R'))
install.packages(c('coda','mvtnorm'))
options(repos=c(getOption('repos'),rethinking='http://xcelab.net/R'))
library(rethinking)
install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
devtools::install_github("rmcelreath/rethinking",ref="Experimental")
install.packages(c("coda","mvtnorm","devtools","loo"))
install.packages(c("coda", "mvtnorm", "devtools", "loo"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")
library(rethinking)
install.packages("Rcpp", repos = "https://rcppcore.github.io/drat")
library(rethinking)
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
Sys.setenv(MAKEFLAGS = "-j4") # four cores used
install.packages("rstan", type = "source")
library(rethinking)
library(rethinking)
install.packages(c("coda","mvtnorm","devtools","loo"))library(devtools)(devtools::install_github("rmcelreath/rethinking")
install.packages(c("coda","mvtnorm","devtools","loo"))
install.packages(c("coda", "mvtnorm", "devtools", "loo"))
library(devtools)(
devtools::install_github("rmcelreath/rethinking")
)
library(devtools)(devtools::install_github("rmcelreath/rethinking"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")
library(rethinking)
library(rethinking)
install.packages('rethinking',type='source')
install.packages(c('devtools','coda','mvtnorm'))
install.packages(c("devtools", "coda", "mvtnorm"))
library(devtools)
install_github("rmcelreath/rethinking")
library(rethinking)
# Run all of the following code, either by selecting all the code and
# pressing the "Run" button (if you are using RStudio) or by copying
# the code and pasting it into the R console and pressing return.
# This bit of code installs the "rstan" package, which is essential to
# the Stan programming language that will do the heavy lifting in the
# background later in the course. Without running this installer first,
# the "rethinking" package won't work properly.
remove.packages("rstan")
install.packages("StanHeaders")
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
pkgbuild::has_build_tools(debug = TRUE)
# Once "rstan" is installed, we can install the "rethinking" package
# proper, drawn directly from the author's website.
install.packages(c("mvtnorm","loo","coda"), repos="https://cloud.r-project.org/",dependencies=TRUE)
options(repos=c(getOption('repos'), rethinking='http://xcelab.net/R'))
install.packages('rethinking',type='source')
library("rstan")
library("rethinking")
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
# If everything has gone according to plan, you should now be able
# to to use the full scope of the tools in the rethinking package.
# If you run into difficulty, let me know ASAP so we can
# troubleshoot the problem.
library(rethinking)
install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
devtools::install_github("rmcelreath/rethinking",ref="Experimental")
install.packages(c("coda", "mvtnorm", "devtools"))
library(rethinking)
install.packages("Rcpp", repos = "https://rcppcore.github.io/drat")
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
Sys.setenv(MAKEFLAGS = "-j4") # four cores used
install.packages("rstan", type = "source")
library(rethinking)
library(rstan)
library(rethinking)
install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
devtools::install_github("rmcelreath/rethinking",ref="Experimental")
install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
devtools::install_github("rmcelreath/rethinking",ref="Experimental")
library(rethinking)
install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
devtools::install_github("rmcelreath/rethinking",ref="Experimental")
install.packages(c("coda", "mvtnorm", "devtools"))
library(rethinking)
library(rethinking)
library(readr)
library(ggplot2)
setwd("~/Documents/Google Drive/Reed/Learning & Adaptive Behavior Laboratory/Sharing Data - Greg")
Behav <- read_csv("Raw_data.csv")
Behav$Subject <- coerce_index(Behav$Rat)
rstan_options(auto_write = TRUE);
options(mc.cores = parallel::detectCores());
pair_machine_p <- stan_model(file="pair_model.stan")
choice_output <- list()
cq <- list()
cyc <- 2000
for (s in unique(Behav$Subject)) {
dex <- (Behav$Condition!=5)&(Behav$Subject==s)
dat <- list(S=sum(dex),C=length(unique(Behav$Condition[dex])),N=Behav$Social[dex]+Behav$Food[dex],condID=Behav$Condition[dex],F=Behav$Food[dex])
dat$condID[dat$condID>5] <- dat$condID[dat$condID>5]-1
choice_output[s] <- list(sampling(pair_machine_p, data=dat, iter=cyc*2, warmup=cyc, chains=4, cores=4, control=list(adapt_delta=0.99,max_treedepth=15)))
cq[s] <- list(extract.samples(choice_output[[s]]))
cq[[s]]$prob <- inv_logit(cq[[s]]$intercept)
}
install.packages("survival")
intake_machine_p <- stan_model(file="intake_model.stan")
intake_output <- list()
iq <- list()
cyc <- 2000
for (s in unique(Behav$Subject)) {
dex <- (Behav$Subject==s)
dat <- list(S=sum(dex),C=length(unique(Behav$Condition[dex])),condID=Behav$Condition[dex],P=Behav$Food[dex]*Behav$FoodAmmt[dex],Shr=Behav$Sharing[dex],Lft=Behav$PelletsLeft[dex])
# As noted in the model, NP is the sum of shared and 'left behind' pellets. Since these two categories of event
# are non-overlapping across conditions, we don't run any risk in pooling them as a global event and then
# teasing the implications apart when we label the results.
dat$Shr[dat$Shr<0] <- 0
dat$Lft[dat$Lft<0] <- 0
dat$NP <- dat$Shr + dat$Lft
intake_output[s] <- list(sampling(intake_machine_p, data=dat, iter=cyc*2, warmup=cyc, chains=4, cores=1, control=list(adapt_delta=0.99,max_treedepth=15)))
iq[s] <- list(extract.samples(intake_output[[s]]))
# Since intercept[i] and self[i] covary, we should *not* interpret each separately. A much safer approach is to
# create a derived value here called "is" that assembles the parameters in the way they are assembled by the model.
# This then lets us obtain "pellets_left" and "pellets_taken" as quantities that correct for the implicit transformation
# of the parameters in the model.
iq[[s]]$is <- iq[[s]]$intercept + iq[[s]]$self
iq[[s]]$pellets_left <- exp(iq[[s]]$intercept)
iq[[s]]$pellets_taken <- exp(iq[[s]]$is)
}
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
iq[[s+1]]$self <- matrix(0,nrow(iq[[1]]$self),ncol(iq[[1]]$self))
iq[[s+1]]$is <- matrix(0,nrow(iq[[1]]$is),ncol(iq[[1]]$is))
for (k in 1:s) {
iq[[s+1]]$intercept <- iq[[s+1]]$intercept + iq[[k]]$intercept/s
iq[[s+1]]$self <- iq[[s+1]]$self + iq[[k]]$self/s
iq[[s+1]]$is <- iq[[s+1]]$is + iq[[k]]$is/s
}
iq[[s+1]]$pellets_left <- exp(iq[[s+1]]$intercept)
iq[[s+1]]$pellets_taken <- exp(iq[[s+1]]$is)
plotdata <- data.frame(
name = c( rep("Cond 1 NP",8000), rep("Cond 1 P",8000), rep("Cond 2 NP",8000), rep("Cond 2 P",8000), rep("Cond 3 NP",8000), rep("Cond 3 P",8000), rep("Cond 4 NP",8000), rep("Cond 4 P",8000), rep("Cond 5 NP",8000), rep("Cond 5 P",8000), rep("Cond 6 NP",8000), rep("Cond 6 P",8000), rep("Cond 7 NP",8000), rep("Cond 7 P",8000) ),
intercept = c( iq[[s+1]]$intercept[,1], iq[[s+1]]$is[,1], iq[[s+1]]$intercept[,2], iq[[s+1]]$is[,2], iq[[s+1]]$intercept[,3], iq[[s+1]]$is[,3], iq[[s+1]]$intercept[,4], iq[[s+1]]$is[,4], iq[[s+1]]$intercept[,5], iq[[s+1]]$is[,5], iq[[s+1]]$intercept[,6], iq[[s+1]]$is[,6], iq[[s+1]]$intercept[,7], iq[[s+1]]$is[,7] ),
condition = c( rep("Cond 1",16000), rep("Cond 2",16000), rep("Cond 3",16000), rep("Cond 4",16000), rep("Cond 5",16000), rep("Cond 6",16000), rep("Cond 7",16000) )
)
p <- ggplot(plotdata, aes(x=name, y=intercept, fill=condition)) +
geom_violin(scale="width")
print(p)
plotdata <- data.frame(
name = c( rep("Cond 1 NP",8000), rep("Cond 1 P",8000), rep("Cond 2 NP",8000), rep("Cond 2 P",8000), rep("Cond 3 NP",8000), rep("Cond 3 P",8000), rep("Cond 4 NP",8000), rep("Cond 4 P",8000), rep("Cond 5 NP",8000), rep("Cond 5 P",8000), rep("Cond 6 NP",8000), rep("Cond 6 P",8000), rep("Cond 7 NP",8000), rep("Cond 7 P",8000) ),
pellets = c( iq[[s+1]]$pellets_left[,1], iq[[s+1]]$pellets_taken[,1], iq[[s+1]]$pellets_left[,2], iq[[s+1]]$pellets_taken[,2], iq[[s+1]]$pellets_left[,3], iq[[s+1]]$pellets_taken[,3], iq[[s+1]]$pellets_left[,4], iq[[s+1]]$pellets_taken[,4], iq[[s+1]]$pellets_left[,5], iq[[s+1]]$pellets_taken[,5], iq[[s+1]]$pellets_left[,6], iq[[s+1]]$pellets_taken[,6], iq[[s+1]]$pellets_left[,7], iq[[s+1]]$pellets_taken[,7] ),
condition = c( rep("Cond 1",16000), rep("Cond 2",16000), rep("Cond 3",16000), rep("Cond 4",16000), rep("Cond 5",16000), rep("Cond 6",16000), rep("Cond 7",16000) )
)
p <- ggplot(plotdata, aes(x=name, y=pellets, fill=condition)) +
geom_violin(scale="width")
print(p)
#====================
#====Front Matter====
#====================
#
# In this section, you're going to want to modify the working directory to reflect wherever the data bundle is located
# on your hard drive. If the read_csv isn't yielding an object in memory that looks like a dataset, that's a sign that
# you haven't set the working directory successfully.
#
library(rethinking)
library(readr)
library(ggplot2)
setwd("~/Documents/Google Drive/Reed/Learning & Adaptive Behavior Laboratory/Sharing Data - Greg")
Behav <- read_csv("Raw_data.csv")
Behav$Subject <- coerce_index(Behav$Rat)
# This bit of code tells R to write the compiled R code to the hard drive. This means you won't need to recompile
# the model every time you run the analysis.
rstan_options(auto_write = TRUE);
options(mc.cores = parallel::detectCores());
intake_machine_p <- stan_model(file="intake_model.stan")
intake_output <- list()
iq <- list()
cyc <- 2000
for (s in unique(Behav$Subject)) {
dex <- (Behav$Subject==s)
dat <- list(S=sum(dex),C=length(unique(Behav$Condition[dex])),condID=Behav$Condition[dex],P=Behav$Food[dex]*Behav$FoodAmmt[dex],Shr=Behav$Sharing[dex],Lft=Behav$PelletsLeft[dex])
# As noted in the model, NP is the sum of shared and 'left behind' pellets. Since these two categories of event
# are non-overlapping across conditions, we don't run any risk in pooling them as a global event and then
# teasing the implications apart when we label the results.
dat$Shr[dat$Shr<0] <- 0
dat$Lft[dat$Lft<0] <- 0
dat$NP <- dat$Shr + dat$Lft
intake_output[s] <- list(sampling(intake_machine_p, data=dat, iter=cyc*2, warmup=cyc, chains=4, cores=1, control=list(adapt_delta=0.99,max_treedepth=15)))
iq[s] <- list(extract.samples(intake_output[[s]]))
# Since intercept[i] and self[i] covary, we should *not* interpret each separately. A much safer approach is to
# create a derived value here called "is" that assembles the parameters in the way they are assembled by the model.
# This then lets us obtain "pellets_left" and "pellets_taken" as quantities that correct for the implicit transformation
# of the parameters in the model.
iq[[s]]$is <- iq[[s]]$intercept + iq[[s]]$self
iq[[s]]$pellets_left <- exp(iq[[s]]$intercept)
iq[[s]]$pellets_taken <- exp(iq[[s]]$is)
}
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
iq[[s+1]]$self <- matrix(0,nrow(iq[[1]]$self),ncol(iq[[1]]$self))
iq[[s+1]]$is <- matrix(0,nrow(iq[[1]]$is),ncol(iq[[1]]$is))
for (k in 1:s) {
iq[[s+1]]$intercept <- iq[[s+1]]$intercept + iq[[k]]$intercept/s
iq[[s+1]]$self <- iq[[s+1]]$self + iq[[k]]$self/s
iq[[s+1]]$is <- iq[[s+1]]$is + iq[[k]]$is/s
}
iq[[s+1]]$pellets_left <- exp(iq[[s+1]]$intercept)
iq[[s+1]]$pellets_taken <- exp(iq[[s+1]]$is)
plotdata <- data.frame(
name = c( rep("Cond 1 NP",8000), rep("Cond 1 P",8000), rep("Cond 2 NP",8000), rep("Cond 2 P",8000), rep("Cond 3 NP",8000), rep("Cond 3 P",8000), rep("Cond 4 NP",8000), rep("Cond 4 P",8000), rep("Cond 5 NP",8000), rep("Cond 5 P",8000), rep("Cond 6 NP",8000), rep("Cond 6 P",8000), rep("Cond 7 NP",8000), rep("Cond 7 P",8000) ),
intercept = c( iq[[s+1]]$intercept[,1], iq[[s+1]]$is[,1], iq[[s+1]]$intercept[,2], iq[[s+1]]$is[,2], iq[[s+1]]$intercept[,3], iq[[s+1]]$is[,3], iq[[s+1]]$intercept[,4], iq[[s+1]]$is[,4], iq[[s+1]]$intercept[,5], iq[[s+1]]$is[,5], iq[[s+1]]$intercept[,6], iq[[s+1]]$is[,6], iq[[s+1]]$intercept[,7], iq[[s+1]]$is[,7] ),
condition = c( rep("Cond 1",16000), rep("Cond 2",16000), rep("Cond 3",16000), rep("Cond 4",16000), rep("Cond 5",16000), rep("Cond 6",16000), rep("Cond 7",16000) )
)
p <- ggplot(plotdata, aes(x=name, y=intercept, fill=condition)) +
geom_violin(scale="width")
print(p)
plotdata <- data.frame(
name = c( rep("Cond 1 NP",8000), rep("Cond 1 P",8000), rep("Cond 2 NP",8000), rep("Cond 2 P",8000), rep("Cond 3 NP",8000), rep("Cond 3 P",8000), rep("Cond 4 NP",8000), rep("Cond 4 P",8000), rep("Cond 5 NP",8000), rep("Cond 5 P",8000), rep("Cond 6 NP",8000), rep("Cond 6 P",8000), rep("Cond 7 NP",8000), rep("Cond 7 P",8000) ),
pellets = c( iq[[s+1]]$pellets_left[,1], iq[[s+1]]$pellets_taken[,1], iq[[s+1]]$pellets_left[,2], iq[[s+1]]$pellets_taken[,2], iq[[s+1]]$pellets_left[,3], iq[[s+1]]$pellets_taken[,3], iq[[s+1]]$pellets_left[,4], iq[[s+1]]$pellets_taken[,4], iq[[s+1]]$pellets_left[,5], iq[[s+1]]$pellets_taken[,5], iq[[s+1]]$pellets_left[,6], iq[[s+1]]$pellets_taken[,6], iq[[s+1]]$pellets_left[,7], iq[[s+1]]$pellets_taken[,7] ),
condition = c( rep("Cond 1",16000), rep("Cond 2",16000), rep("Cond 3",16000), rep("Cond 4",16000), rep("Cond 5",16000), rep("Cond 6",16000), rep("Cond 7",16000) )
)
p <- ggplot(plotdata, aes(x=name, y=pellets, fill=condition)) +
geom_violin(scale="width")
print(p)
#====================
#====Front Matter====
#====================
#
# In this section, you're going to want to modify the working directory to reflect wherever the data bundle is located
# on your hard drive. If the read_csv isn't yielding an object in memory that looks like a dataset, that's a sign that
# you haven't set the working directory successfully.
#
library(rethinking)
library(readr)
library(ggplot2)
setwd("~/Documents/Google Drive/Reed/Learning & Adaptive Behavior Laboratory/Sharing Data - Greg")
Behav <- read_csv("Raw_data.csv")
Behav$Subject <- coerce_index(Behav$Rat)
# This bit of code tells R to write the compiled R code to the hard drive. This means you won't need to recompile
# the model every time you run the analysis.
rstan_options(auto_write = TRUE);
options(mc.cores = parallel::detectCores());
#============================================
#====Stan Analysis of Food Intake By Type====
#============================================
#
# For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed
# in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less
# guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one
# case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.
#
intake_machine_p <- stan_model(file="intake_model.stan")
intake_output <- list()
iq <- list()
cyc <- 2000
for (s in unique(Behav$Subject)) {
dex <- (Behav$Subject==s)
dat <- list(S=sum(dex),C=length(unique(Behav$Condition[dex])),condID=Behav$Condition[dex],P=Behav$Food[dex]*Behav$FoodAmmt[dex],Shr=Behav$Sharing[dex],Lft=Behav$PelletsLeft[dex])
# As noted in the model, NP is the sum of shared and 'left behind' pellets. Since these two categories of event
# are non-overlapping across conditions, we don't run any risk in pooling them as a global event and then
# teasing the implications apart when we label the results.
dat$Shr[dat$Shr<0] <- 0
dat$Lft[dat$Lft<0] <- 0
dat$NP <- dat$Shr + dat$Lft
intake_output[s] <- list(sampling(intake_machine_p, data=dat, iter=cyc*2, warmup=cyc, chains=4, cores=1, control=list(adapt_delta=0.99,max_treedepth=15)))
iq[s] <- list(extract.samples(intake_output[[s]]))
# Since intercept[i] and self[i] covary, we should *not* interpret each separately. A much safer approach is to
# create a derived value here called "is" that assembles the parameters in the way they are assembled by the model.
# This then lets us obtain "pellets_left" and "pellets_taken" as quantities that correct for the implicit transformation
# of the parameters in the model.
iq[[s]]$is <- iq[[s]]$intercept + iq[[s]]$self
iq[[s]]$pellets_left <- exp(iq[[s]]$intercept)
iq[[s]]$pellets_taken <- exp(iq[[s]]$is)
}
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
iq[[s+1]]$self <- matrix(0,nrow(iq[[1]]$self),ncol(iq[[1]]$self))
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow = (iq[[1]]$intercept),ncol = (iq[[1]]$intercept))
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
#============================================
#====Stan Analysis of Food Intake By Type====
#============================================
#
# For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed
# in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less
# guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one
# case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.
#
intake_machine_p <- stan_model(file="intake_model.stan")
intake_output <- list()
iq <- list()
cyc <- 2000
for (s in unique(Behav$Subject)) {
dex <- (Behav$Subject==s)
dat <- list(S=sum(dex),C=length(unique(Behav$Condition[dex])),condID=Behav$Condition[dex],P=Behav$Food[dex]*Behav$FoodAmmt[dex],Shr=Behav$Sharing[dex],Lft=Behav$PelletsLeft[dex])
# As noted in the model, NP is the sum of shared and 'left behind' pellets. Since these two categories of event
# are non-overlapping across conditions, we don't run any risk in pooling them as a global event and then
# teasing the implications apart when we label the results.
dat$Shr[dat$Shr<0] <- 0
dat$Lft[dat$Lft<0] <- 0
dat$NP <- dat$Shr + dat$Lft
intake_output[s] <- list(sampling(intake_machine_p, data=dat, iter=cyc*2, warmup=cyc, chains=4, cores=1, control=list(adapt_delta=0.99,max_treedepth=15)))
iq[s] <- list(extract.samples(intake_output[[s]]))
# Since intercept[i] and self[i] covary, we should *not* interpret each separately. A much safer approach is to
# create a derived value here called "is" that assembles the parameters in the way they are assembled by the model.
# This then lets us obtain "pellets_left" and "pellets_taken" as quantities that correct for the implicit transformation
# of the parameters in the model.
iq[[s]]$is <- iq[[s]]$intercept + iq[[s]]$self
iq[[s]]$pellets_left <- exp(iq[[s]]$intercept)
iq[[s]]$pellets_taken <- exp(iq[[s]]$is)
}
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
iq[[s+1]]$self <- matrix(0,nrow(iq[[1]]$self),ncol(iq[[1]]$self))
iq[[s+1]]$is <- matrix(0,nrow(iq[[1]]$is),ncol(iq[[1]]$is))
#============================================
#====Stan Analysis of Food Intake By Type====
#============================================
#
# For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed
# in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less
# guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one
# case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.
#
intake_machine_p <- stan_model(file="intake_model.stan")
#============================================
#====Stan Analysis of Food Intake By Type====
#============================================
#
# For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed
# in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less
# guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one
# case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.
#
intake_machine_p <- stan_model(file="intake_model.stan")
#============================================
#====Stan Analysis of Food Intake By Type====
#============================================
#
# For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed
# in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less
# guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one
# case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.
#
intake_machine_p <- stan_model(file="intake_model.stan")
#============================================
#====Stan Analysis of Food Intake By Type====
#============================================
#
# For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed
# in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less
# guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one
# case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.
#
intake_machine_p <- stan_model(file="intake_model.stan")
intake_output <- list()
#============================================
#====Stan Analysis of Food Intake By Type====
#============================================
#
# For the most part, the logic of this block of code parallels that above. There is, however, a ltitle extra caution needed
# in manipulating the resulting parameters because the intercept abd self parameters in the stan model are more or less
# guaranteed to covary. As such, in order to talk about specific cases, it's important to always example intercept as one
# case and intercept+self as the other case. Self as a parameter alone is going to be counterintuitive to interpret.
#
intake_machine_p <- stan_model(file="intake_model.stan")
intake_output <- list()
iq <- list()
cyc <- 2000
for (s in unique(Behav$Subject)) {
dex <- (Behav$Subject==s)
dat <- list(S=sum(dex),C=length(unique(Behav$Condition[dex])),condID=Behav$Condition[dex],P=Behav$Food[dex]*Behav$FoodAmmt[dex],Shr=Behav$Sharing[dex],Lft=Behav$PelletsLeft[dex])
# As noted in the model, NP is the sum of shared and 'left behind' pellets. Since these two categories of event
# are non-overlapping across conditions, we don't run any risk in pooling them as a global event and then
# teasing the implications apart when we label the results.
dat$Shr[dat$Shr<0] <- 0
dat$Lft[dat$Lft<0] <- 0
dat$NP <- dat$Shr + dat$Lft
intake_output[s] <- list(sampling(intake_machine_p, data=dat, iter=cyc*2, warmup=cyc, chains=4, cores=1, control=list(adapt_delta=0.99,max_treedepth=15)))
iq[s] <- list(extract.samples(intake_output[[s]]))
# Since intercept[i] and self[i] covary, we should *not* interpret each separately. A much safer approach is to
# create a derived value here called "is" that assembles the parameters in the way they are assembled by the model.
# This then lets us obtain "pellets_left" and "pellets_taken" as quantities that correct for the implicit transformation
# of the parameters in the model.
iq[[s]]$is <- iq[[s]]$intercept + iq[[s]]$self
iq[[s]]$pellets_left <- exp(iq[[s]]$intercept)
iq[[s]]$pellets_taken <- exp(iq[[s]]$is)
}
# As before, the parameters are averaged after-the-fact across subjects, rather than being part of a multi-level
# model. This approach is *not* an optimal use of the available data, but it should in principle be unbiased so
# long as subjects were independent samples. It's also much easier to understand the model as it applies to single
# subjects.
iq[s+1] <- list(extract.samples(intake_output[[s]]))
iq[[s+1]]$intercept <- matrix(0,nrow(iq[[1]]$intercept),ncol(iq[[1]]$intercept))
library(rethinking)
library(readr)
library(ggplot2)
library(rstan)
library(dplyr)
library(tidyr)
setwd("~/Documents/Google Drive/Reed/Learning & Adaptive Behavior Laboratory/Sharing Data - Matt")
Behav <- read_csv("Raw_data.csv")
Behav$Subject <- coerce_index(Behav$Rat)
rstan_options(auto_write = TRUE);
options(mc.cores = parallel::detectCores());
model1 <- stan_model(file="pair_model.stan")
choice_output <- list()
sample_parameter_1 <- list()
nsample <- 2000
for (s in unique(Behav$Subject)) {
mdata_1 <- (Behav$Condition != 5) & (Behav$Subject == s)
mdata_2 <- list(S = sum(mdata_1), C = length(unique(Behav$Condition[mdata_1])), N = Behav$Social[mdata_1] + Behav$Food[mdata_1], condID = Behav$Condition[mdata_1], F = Behav$Food[mdata_1])
mdata_2$condID[mdata_2$condID > 5] <- mdata_2$condID[mdata_2$condID > 5] - 1
choice_output[s] <- list(sampling(model1, data = mdata_2, iter = nsample*2, warmup = nsample, chains = 4, cores = 1, control=list(adapt_delta = 0.99, max_treedepth = 15)))
sample_parameter_1[s] <- list(extract.samples(choice_output[[s]]))
sample_parameter_1[[s]]$prob <- inv_logit(sample_parameter_1[[s]]$intercept)
}
